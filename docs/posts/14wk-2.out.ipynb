{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14wk-2: 강화학습 (3) – 4x4 Grid World (`AgentGreedy`,`AgentExplorer`)\n",
        "\n",
        "최규빈  \n",
        "2024-06-06\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/14wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>\n",
        "\n",
        "# 1. 강의영상\n",
        "\n",
        "<https://youtu.be/playlist?list=PLQqh36zP38-wScTnw_5pcwdCabtqh1aAP&si=DZEC0ylFQhw1Ufjo>\n",
        "\n",
        "# 2. Imports"
      ],
      "id": "dc37904d-95ec-4d8a-9ad2-3a2f122f5d47"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "#!pip install gymnasium\n",
        "#---#\n",
        "import gymnasium as gym\n",
        "#---#\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import IPython"
      ],
      "id": "5b1cfd48-d338-4ddd-8b59-f8ab593ebda9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 필요한 클래스 및 함수선언"
      ],
      "id": "d19b2e8b-685b-4992-8bf2-fe12b6a188fb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "action_to_direction = {\n",
        "    0 : np.array([1, 0]), # row+, down\n",
        "    1 : np.array([0, 1]), # col+, right\n",
        "    2 : np.array([-1 ,0]), # row-, up\n",
        "    3 : np.array([0, -1]) # col-, left\n",
        "}\n",
        "action_to_direction2 = {0: 'down', 1: 'right', 2: 'up', 3: 'left'} # 당장쓰진 않지만 하는김에 "
      ],
      "id": "23c93695"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "def show(states):\n",
        "    fig = plt.Figure()\n",
        "    ax = fig.subplots()\n",
        "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
        "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
        "    ax.text(0, 0, 'start', ha='center', va='center')\n",
        "    ax.text(3, 3, 'end', ha='center', va='center')\n",
        "    # Adding grid lines to the plot\n",
        "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
        "    state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "    def update(t):\n",
        "        if states[t] in state_space:\n",
        "            s1,s2 = states[t]\n",
        "            states[t] = [s2,s1]\n",
        "            sc.set_offsets(states[t])\n",
        "        else:\n",
        "            s1,s2 = states[t]\n",
        "            s1 = s1 + 0.5 if s1 < 0 else (s1 - 0.5 if s1 > 3 else s1)\n",
        "            s2 = s2 + 0.5 if s2 < 0 else (s2 - 0.5 if s2 > 3 else s2)\n",
        "            states[t] = [s2,s1]       \n",
        "            sc.set_offsets(states[t])\n",
        "    ani = FuncAnimation(fig,update,frames=len(states))\n",
        "    display(IPython.display.HTML(ani.to_jshtml()))"
      ],
      "id": "1ab7c3d4-811b-466b-927f-fed8d8e87b51"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.state_space = gym.spaces.MultiDiscrete([4,4])\n",
        "        self.action_space = gym.spaces.Discrete(4) \n",
        "        self._action_to_direction = {\n",
        "            0 : np.array([1, 0]), # row+, down\n",
        "            1 : np.array([0, 1]), # col+, right\n",
        "            2 : np.array([-1 ,0]), # row-, up\n",
        "            3 : np.array([0, -1]) # col-, left\n",
        "        }\n",
        "        self.reset()\n",
        "        self.state = None \n",
        "        self.reward = None \n",
        "        self.termiated = None\n",
        "    def step(self,action):\n",
        "        direction = self._action_to_direction[action]\n",
        "        self.state = self.state + direction\n",
        "        if np.array_equal(self.state,np.array([3,3])): \n",
        "            self.reward = 100 \n",
        "            self.terminated = True\n",
        "        elif self.state not in self.state_space:\n",
        "            self.reward = -10\n",
        "            self.terminated = True\n",
        "        else:\n",
        "            self.reward = -1 \n",
        "        return self.state, self.reward, self.terminated\n",
        "    def reset(self):\n",
        "        self.state = np.array([0,0])\n",
        "        self.terminated = False   \n",
        "        return self.state "
      ],
      "id": "e89495dc"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentRandom: \n",
        "    def __init__(self,env):\n",
        "        #--# define spaces \n",
        "        self.action_space = env.action_space\n",
        "        self.state_space = env.state_space\n",
        "        #--# replay buffer \n",
        "        self.action = None \n",
        "        self.actions = [] \n",
        "        self.current_state =  None \n",
        "        self.current_states = [] \n",
        "        self.reward = None \n",
        "        self.rewards = [] \n",
        "        self.next_state =  None \n",
        "        self.next_states = [] \n",
        "        self.terminated = None \n",
        "        self.terminations = []\n",
        "        #--# other information\n",
        "        self.n_episodes = 0         \n",
        "        self.n_experiences = 0\n",
        "        self.score = 0        \n",
        "        self.playtimes = [] \n",
        "        self.scores = []    \n",
        "    def act(self):\n",
        "        self.action = self.action_space.sample()\n",
        "    def learn(self):\n",
        "        pass \n",
        "    def save_experience(self):\n",
        "        self.current_states.append(self.current_state)        \n",
        "        self.actions.append(self.action)\n",
        "        self.rewards.append(self.reward)  \n",
        "        self.next_states.append(self.next_state)\n",
        "        self.terminations.append(self.terminated)\n",
        "        #--#\n",
        "        self.n_experiences = self.n_experiences + 1 \n",
        "        self.score = self.score + self.reward"
      ],
      "id": "b6a2a93a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. `AgentGreedy`\n",
        "\n",
        "## A. 환경의 이해\n",
        "\n",
        "`-` 랜덤에이전트를 이용해 무작위로 10000판을 진행해보자."
      ],
      "id": "141dcd06-c326-4f65-b378-d14700d134bb"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "env = GridWorld()\n",
        "agent = AgentRandom(env) \n",
        "for _ in range(10000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에피소드 진행 \n",
        "    for t in range(1,51):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn() # 사실학습하는 함수는 dummy 함수임..\n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t)\n",
        "    agent.n_episodes = agent.n_episodes + 1 "
      ],
      "id": "40b12099-567e-419d-b629-d34f01810511"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.n_experiences"
      ],
      "id": "ceba972e-030e-4bd1-ae1d-5367cea52107"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 데이터관찰"
      ],
      "id": "1409058a-64b0-468b-85fa-61cc972fa44b"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "agent.rewards[0], agent.next_states[0]"
      ],
      "id": "0654242d-704d-4e69-b1f5-792dcb7e9efd"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / (3, 'left')\n",
            "환경: 보상/다음상태 = -10 / [ 0 -1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[0]} / {agent.actions[0],action_to_direction2[agent.actions[0]]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[0]} / {agent.next_states[0]}\")"
      ],
      "id": "907a43bc-1644-4ad2-ba8f-f9296035721a"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / (2, 'up')\n",
            "환경: 보상/다음상태 = -10 / [-1  0]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[1]} / {agent.actions[1],action_to_direction2[agent.actions[1]]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[1]} / {agent.next_states[1]}\")"
      ],
      "id": "57ae01f6-3178-477f-ba2b-b708a91338c4"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / (2, 'up')\n",
            "환경: 보상/다음상태 = -10 / [-1  0]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[2]} / {agent.actions[2],action_to_direction2[agent.actions[2]]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[2]} / {agent.next_states[2]}\")"
      ],
      "id": "9fdc44e3-02df-4b0b-a4e4-39765d50e7c2"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / (3, 'left')\n",
            "환경: 보상/다음상태 = -10 / [ 0 -1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[3]} / {agent.actions[3],action_to_direction2[agent.actions[3]]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[3]} / {agent.next_states[3]}\")"
      ],
      "id": "69c21c21-3995-4167-8741-becd4ac9a2a1"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 0] / (1, 'right')\n",
            "환경: 보상/다음상태 = -1 / [0 1]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[4]} / {agent.actions[4],action_to_direction2[agent.actions[4]]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[4]} / {agent.next_states[4]}\")"
      ],
      "id": "4288e684-decd-4056-83bd-9f2a0b79ec35"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 1] / (1, 'right')\n",
            "환경: 보상/다음상태 = -1 / [0 2]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[5]} / {agent.actions[5],action_to_direction2[agent.actions[5]]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[5]} / {agent.next_states[5]}\")"
      ],
      "id": "82082e90-7b41-4817-953f-f8401e208b64"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에이전트: 현재상태/행동 = [0 2] / (2, 'up')\n",
            "환경: 보상/다음상태 = -10 / [-1  2]"
          ]
        }
      ],
      "source": [
        "print(f\"에이전트: 현재상태/행동 = {agent.current_states[6]} / {agent.actions[6],action_to_direction2[agent.actions[6]]}\")\n",
        "print(f\"환경: 보상/다음상태 = {agent.rewards[6]} / {agent.next_states[6]}\")"
      ],
      "id": "93c57519-1763-4463-b693-72a0cea8f593"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (1)"
      ],
      "id": "8d69485a-947b-493c-ab2d-078df0ca2c5b"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4]) \n",
        "count = np.zeros([4,4,4])\n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    r = agent.rewards[i]\n",
        "    q_table[s1,s2,a] = q_table[s1,s2,a] + r \n",
        "    count[s1,s2,a] = count[s1,s2,a] + 1 "
      ],
      "id": "d93824ff-f3a4-4f4c-9d49-2bda502109e7"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[0,0,:]"
      ],
      "id": "32e64326-55d2-422b-a71e-c9c201a4542d"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "count[count==0] = 0.01\n",
        "q_table = q_table / count "
      ],
      "id": "bb284153-2bb4-47a9-a1e5-4df1cf57063b"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            "[[ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1. 100.]\n",
            " [-10. -10. -10.   0.]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            "[[ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1.  -1. -10.]\n",
            " [ -1.  -1. 100.   0.]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            "[[-10. -10. -10. -10.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.  -1.]\n",
            " [ -1.  -1.  -1.   0.]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            "[[-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.  -1.]\n",
            " [-10.  -1.  -1.   0.]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(f\"action = {i}/{action_to_direction2[i]}\")\n",
        "    print(f\"action-value function = \\n{q_table[:,:,i]}\\n\")"
      ],
      "id": "66ec21b2-929f-4886-af3c-8b854862ef0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 환경을 이해하기 위한 기록 (2)"
      ],
      "id": "eab0f495-39ed-4494-95cc-d4af5bc719b3"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4]) \n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    a = agent.actions[i]\n",
        "    r = agent.rewards[i]\n",
        "    q_hat = q_table[s1,s2,a] # 우리가 환경을 이해해서 얻은값, 우리가 풀어낸 답 \n",
        "    q = r # 실제답 \n",
        "    diff = q - q_hat # 실제답과 풀이한값의 차이 = 오차피드백값\n",
        "    q_table[s1,s2,a] = q_hat + 0.05 * diff "
      ],
      "id": "154958ee-8938-421a-bcb6-e1cd9dd2a7db"
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            "[[ -1.    -1.    -1.    -1.  ]\n",
            " [ -1.    -1.    -1.    -1.  ]\n",
            " [ -1.    -1.    -1.    98.72]\n",
            " [-10.    -9.99  -9.91   0.  ]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            "[[-1.   -1.   -1.   -9.99]\n",
            " [-1.   -1.   -1.   -9.99]\n",
            " [-1.   -1.   -1.   -9.93]\n",
            " [-1.   -1.   98.9   0.  ]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            "[[-10.   -10.   -10.    -9.97]\n",
            " [ -1.    -1.    -1.    -1.  ]\n",
            " [ -1.    -1.    -1.    -0.99]\n",
            " [ -1.    -1.    -0.99   0.  ]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            "[[-10.    -1.    -1.    -1.  ]\n",
            " [-10.    -1.    -1.    -1.  ]\n",
            " [-10.    -1.    -1.    -0.98]\n",
            " [ -9.98  -1.    -0.99   0.  ]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(f\"action = {i}/{action_to_direction2[i]}\")\n",
        "    print(f\"action-value function = \\n{q_table[:,:,i].round(2)}\\n\")"
      ],
      "id": "82b87ed5-3c3b-446f-9c8f-6872775b3238"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경의 깊은 이해\n",
        "\n",
        "`-` 분석1: row=3, col=2 상태에서 행동1(right)에 대한 가치"
      ],
      "id": "6131509c-f4ce-40dd-9fe5-7a4c759b2a96"
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,2,1]"
      ],
      "id": "692a6fc1-c605-4fae-b273-ec48d01b0e2e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,2)에서 행동1을 하게되면 100의 보상을 얻으므로\n",
        "    `q_table[3,2,1] = 98.904` 는 합리적임\n",
        "\n",
        "`-` 분석2: row=3, col=1 상태에서 행동1에(right)에 대한 가치"
      ],
      "id": "58f0b45f-0174-4b2c-8a67-c9c50a17b8f5"
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[3,1,1]"
      ],
      "id": "0edd27c6-05b1-450d-bbee-4dcca6bb42c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   상태 (3,1)에서 행동1을 하게되면 -1 의 보상을 얻으므로\n",
        "    `q_table[3,1,1] = - 0.999` 는 합리적인가??\n",
        "\n",
        "`-` 비판: 분석2는 합리적인것 처럼 보이지만 `data`를 분석한 뒤에는 그다지\n",
        "합리적이지 못함.\n",
        "\n",
        "`-` 상황상상\n",
        "\n",
        "-   빈 종이를 줌\n",
        "-   빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 혹은 1)\n",
        "-   0을 쓸때와 1을 쓸때 보상이 다름\n",
        "-   무수히 많은 데이터를 분석해보니, 0을 쓰면 0원을 주고 1을 쓰면\n",
        "    10만원을 보상을 준다는 것을 “알게 되었음”\n",
        "-   이때 빈 종이의 가치는 5만원인가? 10만원인가? –\\> 거의 10만원아니야?\n",
        "    (9.99만원쯤?)\n",
        "\n",
        "`-` 직관: 생각해보니 현재 $s=(3,1)$ $a=1$에서 추정된(esitated) 값은\n",
        "`q_table[3,1,1]` $\\approx$ -1 이지만[1], 현실적으로는 “실제보상(-1)과\n",
        "잠재적보상(100)”을 동시에 고려해야 하는게 합리적임\n",
        "\n",
        "[1] 즉 next_state가 가지는 잠재적값어치는 고려되어있지 않음"
      ],
      "id": "f7c874c0-e870-4742-ae7d-3a6f0dbb86ce"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_hat = q_table[3,1,1]\n",
        "q_hat"
      ],
      "id": "b9b96783-5427-4a86-acbd-afc71e094362"
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q = (-1) + 0.99 * 100 \n",
        "q"
      ],
      "id": "46d584e6-cbb7-409b-8d4f-af4678ad5184"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   여기에서 0.99는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를\n",
        "    결정하는 가중치” 이다.\n",
        "-   1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이\n",
        "    $\\approx$ 십만원 으로 생각한다는 의미)\n",
        "-   0.99는 보통 $\\gamma$라는 기호로 표기하며 `discount rate`이라고\n",
        "    표현한다. (외우세여)\n",
        "\n",
        "`-` 즉 $q(s,a)$는 모든 $s$, $a$에 대하여\n",
        "\n",
        "$$q(s,a) \\approx \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a)$$\n",
        "\n",
        "가 성립한다면 $q(s,a)$는 타당하게 추정된 것이라 볼 수 있다. 물론 수식을\n",
        "좀 더 엄밀하게 쓰면 (terminated, not-terminated 로 나누어 쓰면) 아래와\n",
        "같다.\n",
        "\n",
        "$$q(s,a) \\approx \\begin{cases}  \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a) & \\text{not terminated} \\\\ \\text{reward}(s,a) & \\text{terminated} \\end{cases}$$\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> 대충 설명하면서 넘어갔지만 이 수식을 **벨만방정식**이라고 부른다.\n",
        "> (외우세여) 위의 식은 강화학습에서 가장 중요한 식이며 원래 버전은\n",
        "> 아래와 같다.\n",
        ">\n",
        "> $$Q^\\star(s,a) = R(s,a) +\\gamma\\sum_{s'}P(s'|s,a)\\max_{a}Q(s',a)$$\n",
        ">\n",
        "> 여기에서 $P(s'|s,a)$ 는 상태 $s \\in {\\cal S}$에서 행동\n",
        "> $a \\in {\\cal A}$를 했을때 $s'$에 있을 확률이다. 이러한 확률은\n",
        "> “바람,소용돌이” 등의 외부의 확률적인 요소가 있는 환경에서 의미가\n",
        "> 있으며 우리의 예제에서는 의미가 없다."
      ],
      "id": "066a1c91-4e82-47f7-86ad-fa22d9b0a84d"
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table = np.zeros([4,4,4]) \n",
        "for i in range(agent.n_experiences):\n",
        "    s1,s2 = agent.current_states[i]\n",
        "    ss1,ss2 = agent.next_states[i]\n",
        "    a = agent.actions[i]\n",
        "    r = agent.rewards[i]\n",
        "    q_hat = q_table[s1,s2,a] # 우리가 환경을 이해해서 얻은값, 우리가 풀어낸 답 \n",
        "    if agent.terminations[i]:\n",
        "        q = r \n",
        "    else:\n",
        "        future_reward = q_table[ss1,ss2,:].max()\n",
        "        q = r + 0.99 * future_reward \n",
        "    diff = q - q_hat # 실제답과 풀이한값의 차이 = 오차피드백값\n",
        "    q_table[s1,s2,a] = q_hat + 0.05 * diff "
      ],
      "id": "adc233eb-8174-4149-bdc8-96631794991b"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action = 0/down\n",
            "action-value function = \n",
            "[[ 87.57  89.5   91.05  89.46]\n",
            " [ 89.25  91.56  93.71  95.09]\n",
            " [ 84.44  91.63  96.13  98.72]\n",
            " [-10.    -9.99  -9.91   0.  ]]\n",
            "\n",
            "action = 1/right\n",
            "action-value function = \n",
            "[[87.55 88.89 86.05 -9.99]\n",
            " [89.51 91.48 92.34 -9.99]\n",
            " [91.28 93.68 96.   -9.93]\n",
            " [87.7  94.52 98.9   0.  ]]\n",
            "\n",
            "action = 2/up\n",
            "action-value function = \n",
            "[[-10.   -10.   -10.    -9.97]\n",
            " [ 85.52  87.25  88.12  80.83]\n",
            " [ 86.99  88.99  90.17  86.96]\n",
            " [ 85.85  88.51  89.37   0.  ]]\n",
            "\n",
            "action = 3/left\n",
            "action-value function = \n",
            "[[-10.    85.5   86.94  84.25]\n",
            " [-10.    87.39  88.97  89.31]\n",
            " [-10.    88.83  90.9   86.27]\n",
            " [ -9.98  80.23  81.86   0.  ]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print(f\"action = {i}/{action_to_direction2[i]}\")\n",
        "    print(f\"action-value function = \\n{q_table[:,:,i].round(2)}\\n\")"
      ],
      "id": "104fba86-7c1d-4930-be25-a194b4c89795"
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table.max(axis=-1)"
      ],
      "id": "8386b196-aaa4-47cb-8ff9-7bc5452721eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 행동 전략 수립\n",
        "\n",
        "`-` 상태 (0,0)에 있다고 가정해보자."
      ],
      "id": "8f51d891-efc9-4b2c-a168-4bd4bfc5a4b2"
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 87.5672123   87.54804715 -10.         -10.        ]\n",
            "{0: 'down', 1: 'right', 2: 'up', 3: 'left'}"
          ]
        }
      ],
      "source": [
        "print(q_table[0,0,:])\n",
        "print(action_to_direction2)"
      ],
      "id": "acf35e03-ab43-4dba-979f-2c4e6f896caf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0 혹은 행동 1을 하는게 유리하다. // 행동 2,3을 하면 망한다.\n",
        "\n",
        "`-` 상태 (2,3)에 있다고 가정해보자."
      ],
      "id": "009b11f2-8036-4687-9c71-b28a2337c32c"
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[98.72207181 -9.92731143 86.96434157 86.27101599]\n",
            "{0: 'down', 1: 'right', 2: 'up', 3: 'left'}"
          ]
        }
      ],
      "source": [
        "print(q_table[2,3,:])\n",
        "print(action_to_direction2)"
      ],
      "id": "65422c5d-ba10-4d3f-84b1-29a12ecd25a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동 0을 하는게 유리함.\n",
        "\n",
        "`-` 상태 (3,2)에 있다고 가정해보자."
      ],
      "id": "31d98c24-5ef8-4f15-b463-fa865d79bd80"
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-9.90606054 98.90433632 89.37012074 81.86454084]\n",
            "{0: 'down', 1: 'right', 2: 'up', 3: 'left'}"
          ]
        }
      ],
      "source": [
        "print(q_table[3,2,:])\n",
        "print(action_to_direction2)"
      ],
      "id": "5e8de9f9-520d-41f7-8a20-0364eeb1a38a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   행동1을 하는게 유리함\n",
        "\n",
        "`-` 위에서 제시한 각 상태에서 최적은 action은 아래와 같다."
      ],
      "id": "118ba806-2cfe-48b5-aaf2-a6c719dbe769"
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "1"
          ]
        }
      ],
      "source": [
        "print(q_table[0,0,:].argmax())\n",
        "print(q_table[2,3,:].argmax())\n",
        "print(q_table[3,2,:].argmax())"
      ],
      "id": "870f5499-4255-49c4-b733-8ba53320e9aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`-` 전략(=정책)을 정리해보자.\n",
        "\n",
        "(ver1)"
      ],
      "id": "e3e5e995-1e6b-43a9-9b37-3de0eb9d5899"
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table.argmax(axis=-1)"
      ],
      "id": "6fc40dec-661c-45b6-8da5-f24503b87f71"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(ver2)"
      ],
      "id": "4ca8a8ed-d8ba-44a7-8901-358f2addb383"
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = np.array([\"?????\"]*16).reshape(4,4)\n",
        "policy"
      ],
      "id": "4eb4eaaf-be1a-4e1b-8169-f15aee5a4c4e"
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "for s1 in range(4):\n",
        "    for s2 in range(4):\n",
        "        policy[s1,s2] = action_to_direction2[q_table[s1,s2,:].argmax()]\n",
        "policy"
      ],
      "id": "951d925b-d28d-4846-9f43-f27dd5ea1c87"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D. 에이전트 클래스 설계"
      ],
      "id": "2cdb01e7-bd7d-4586-9f67-fec7a59487e0"
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "q_table[0,0,:]"
      ],
      "id": "d845bad9-5905-4d9d-adf1-4dae0ccd3289"
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentGreedy(AgentRandom):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        #--#\n",
        "        self.q_table = np.zeros([4,4,4])\n",
        "    def learn(self): # q_table \n",
        "        s1,s2 = self.current_state\n",
        "        ss1,ss2 = self.next_state\n",
        "        a = self.action\n",
        "        r = self.reward\n",
        "        q_hat = self.q_table[s1,s2,a] # 우리가 환경을 이해해서 얻은값, 우리가 풀어낸 답 \n",
        "        if self.terminated:\n",
        "            q = r \n",
        "        else:\n",
        "            future_reward = self.q_table[ss1,ss2,:].max()\n",
        "            q = r + 0.99 * future_reward \n",
        "        diff = q - q_hat\n",
        "        self.q_table[s1,s2,a] = q_hat + 0.05 * diff         \n",
        "    def act(self):\n",
        "        if self.n_experiences < 3000:\n",
        "            self.action = self.action_space.sample()\n",
        "        else: \n",
        "            s1,s2 = self.current_state \n",
        "            self.action = self.q_table[s1,s2,:].argmax() # 그리디.."
      ],
      "id": "a2cce582-bd94-4a9c-a9b4-09fbfacbe5cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E. 환경과 상호작용"
      ],
      "id": "514d20c5-f7ba-42f3-8fdc-48e0df0bdb78"
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에피소드:300    점수(에피소드):-7.32  게임시간(에피소드):3.45 \n",
            "에피소드:600    점수(에피소드):-9.92  게임시간(에피소드):3.48 \n",
            "에피소드:900    점수(에피소드):-0.54  게임시간(에피소드):3.64 \n",
            "에피소드:1200   점수(에피소드):95.00  게임시간(에피소드):6.00 \n",
            "에피소드:1500   점수(에피소드):95.00  게임시간(에피소드):6.00 \n",
            "에피소드:1800   점수(에피소드):95.00  게임시간(에피소드):6.00 \n",
            "에피소드:2100   점수(에피소드):95.00  게임시간(에피소드):6.00 \n",
            "에피소드:2400   점수(에피소드):95.00  게임시간(에피소드):6.00 \n",
            "에피소드:2700   점수(에피소드):95.00  게임시간(에피소드):6.00 \n",
            "에피소드:3000   점수(에피소드):95.00  게임시간(에피소드):6.00 "
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = AgentGreedy(env) \n",
        "for _ in range(3000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에피소드 진행 \n",
        "    for t in range(1,51):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn() \n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t)\n",
        "    agent.n_episodes = agent.n_episodes + 1 \n",
        "    #---#\n",
        "    logfreq = 300\n",
        "    if (agent.n_episodes % logfreq) == 0: \n",
        "        print(\n",
        "            f\"에피소드:{agent.n_episodes}\\t\"\n",
        "            f\"점수(에피소드):{np.mean(agent.scores[-logfreq:]):.2f}\\t\"\n",
        "            f\"게임시간(에피소드):{np.mean(agent.playtimes[-logfreq:]):.2f}\\t\"\n",
        "        )"
      ],
      "id": "b2597f42-dcc8-42db-94d7-0685e5afdef1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## F. 상호작용결과 시각화"
      ],
      "id": "b44a4b0e-65af-4261-9632-8490b29e6a83"
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\n",
        "show(states)"
      ],
      "id": "0db95456-989f-4f83-843c-51d4c0ed6e69"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. `AgentExplorer`\n",
        "\n",
        "## A. 클래스 설계"
      ],
      "id": "5beb08ca-d893-40d5-91c8-e5d78cc17764"
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "class AgentExplorer(AgentGreedy):\n",
        "    def __init__(self,env):\n",
        "        super().__init__(env)\n",
        "        self.eps = 0 # 이것이 0이라는 의미는 돌발행동을 안한다는 의미. 즉 AgentGreedy 와 같은 행동을 한다는 의미 \n",
        "    def act(self):\n",
        "        if np.random.rand() < self.eps:\n",
        "            self.action = self.action_space.sample()\n",
        "        else: \n",
        "            super().act() "
      ],
      "id": "854565f0-6d54-4bb4-8376-bafad31fedb7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B. 환경과 상호작용"
      ],
      "id": "2886c9f1-5a2a-4135-b55e-572269e8a410"
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에피소드:300    점수(에피소드):-11.40 게임시간(에피소드):3.50 돌발행동(에피소드):0.74\n",
            "에피소드:600    점수(에피소드):-9.67  게임시간(에피소드):3.60 돌발행동(에피소드):0.55\n",
            "에피소드:900    점수(에피소드):0.70   게임시간(에피소드):4.23 돌발행동(에피소드):0.41\n",
            "에피소드:1200   점수(에피소드):51.58  게임시간(에피소드):6.15 돌발행동(에피소드):0.30\n",
            "에피소드:1500   점수(에피소드):65.10  게임시간(에피소드):6.20 돌발행동(에피소드):0.22\n",
            "에피소드:1800   점수(에피소드):69.62  게임시간(에피소드):6.08 돌발행동(에피소드):0.17\n",
            "에피소드:2100   점수(에피소드):78.86  게임시간(에피소드):6.01 돌발행동(에피소드):0.12\n",
            "에피소드:2400   점수(에피소드):83.88  게임시간(에피소드):6.12 돌발행동(에피소드):0.09\n",
            "에피소드:2700   점수(에피소드):84.49  게임시간(에피소드):5.87 돌발행동(에피소드):0.07\n",
            "에피소드:3000   점수(에피소드):88.74  게임시간(에피소드):6.03 돌발행동(에피소드):0.05"
          ]
        }
      ],
      "source": [
        "env = GridWorld()\n",
        "agent = AgentExplorer(env) \n",
        "agent.eps = 1 # 돌발행동할 확률이 100퍼 \n",
        "for _ in range(3000):\n",
        "    # Step1: 에피소드 준비 \n",
        "    agent.current_state = env.reset()\n",
        "    agent.terminated = False \n",
        "    agent.score = 0 \n",
        "    # Step2: 에피소드 진행 \n",
        "    for t in range(1,51):\n",
        "        # step1: 행동\n",
        "        agent.act() \n",
        "        # step2: 보상 \n",
        "        agent.next_state, agent.reward, agent.terminated = env.step(agent.action)\n",
        "        # step3: 저장 & 학습 \n",
        "        agent.save_experience() \n",
        "        agent.learn() \n",
        "        # step4: 다음 스텝준비 \n",
        "        agent.current_state = agent.next_state \n",
        "        if agent.terminated: break \n",
        "    # Step3: 다음에피소드 준비 \n",
        "    agent.scores.append(agent.score) \n",
        "    agent.playtimes.append(t)\n",
        "    agent.n_episodes = agent.n_episodes + 1 \n",
        "    agent.eps = agent.eps * 0.999\n",
        "    #---#\n",
        "    logfreq = 300\n",
        "    if (agent.n_episodes % logfreq) == 0: \n",
        "        print(\n",
        "            f\"에피소드:{agent.n_episodes}\\t\"\n",
        "            f\"점수(에피소드):{np.mean(agent.scores[-logfreq:]):.2f}\\t\"\n",
        "            f\"게임시간(에피소드):{np.mean(agent.playtimes[-logfreq:]):.2f}\\t\"\n",
        "            f\"돌발행동(에피소드):{agent.eps:.2f}\"\n",
        "        )"
      ],
      "id": "a0ad7443-50d2-49bd-89f7-5a4939807052"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C. 상호작용 결과 시각화"
      ],
      "id": "19b02cc0-8aa9-4469-ae79-c7adba50ddd1"
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:]\n",
        "show(states)"
      ],
      "id": "d2a6c635-87dc-4638-b603-53bea4c77ecc"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  }
}