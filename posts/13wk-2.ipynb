{
 "cells": [
  {
   "cell_type": "raw",
   "id": "855d203d-e1fd-40dc-9c2c-e90e8380f06e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"13wk-2: 강화학습 (1) -- Bandit\"\n",
    "author: \"최규빈\"\n",
    "date: \"05/29/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c13712-1185-4386-90e7-3e23c62be398",
   "metadata": {
    "id": "e67ab8e0"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guebin/DL2024/blob/main/posts/13wk-2.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"text-align: left\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d11f9-7f12-401a-82da-e4b2e568dde6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 강의영상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4685c071-46e9-4bcb-84a7-ece876eb4654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#{{<video https://youtu.be/playlist?list=PLQqh36zP38-zoOHd7w3N5q9Jc5P34Ux8X&si=MdJTHM3a27MCAssp >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad3d76-42f2-4356-9024-e3476f3a1d05",
   "metadata": {},
   "source": [
    "# 2. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf93dfa6-34a4-44ab-8bc5-61b02797dc0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5cf75f-8c30-4b70-9ab7-0a7d6c021c70",
   "metadata": {},
   "source": [
    "# 3. 강화학습 Intro "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b320f1-50e4-43a2-b2e9-69164be99e5e",
   "metadata": {},
   "source": [
    "`-` 강화학습(대충설명): 어떠한 \"(게임)환경\"이 있을때 거기서 \"뭘 할지\"를 학습하는 과업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e2156-f5f4-4ef0-903b-cd0c118fbabf",
   "metadata": {},
   "source": [
    "`-` 딥마인드: breakout $\\to$ 알파고 \n",
    "\n",
    "- <https://www.youtube.com/watch?v=TmPfTpjtdgg>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e915f-145f-4d06-a0b0-283d943e67fd",
   "metadata": {},
   "source": [
    "`-` 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f9bc7-69e7-4d96-8944-e51e028d00da",
   "metadata": {},
   "source": [
    "# 4. Game1: `Bandit` 게임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d173a9-184c-48bc-9634-1cffbca29dd5",
   "metadata": {},
   "source": [
    "## A. 게임설명 및 원시코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49044acf-a128-47d1-877e-6b949db1e044",
   "metadata": {},
   "source": [
    "`-` 문제설명: 두 개의 버튼이 있다. `버튼0`을 누르면 1의 보상을, `버튼1`을 누르면 10의 보상을 준다고 가정 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b39d419-9ac9-4eeb-ad1f-e15647ba959c",
   "metadata": {},
   "source": [
    "`-` 처음에 어떤 행동을 해야 하는가?\n",
    "\n",
    "- 처음에는 아는게 없음\n",
    "- 일단 \"아무거나\" 눌러보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275f51e-d5eb-484d-aef0-bdcef03d8d00",
   "metadata": {},
   "source": [
    "`-` 버튼을 아무거나 누르는 코드를 작성해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b59a9c17-68af-4172-b9a1-a57698c48064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'버튼0'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space = ['버튼0','버튼1']\n",
    "action = np.random.choice(action_space,p=[0.5,0.5])\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229fa9ce-0fac-42ea-a1d7-77c063ed2c1e",
   "metadata": {},
   "source": [
    "> `action_space` 와 `action` 이라는 용어를 기억할 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e48267-1041-40e0-8532-96fb02024c92",
   "metadata": {},
   "source": [
    "`-` 버튼을 누른 행위에 따른 보상을 구현하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "284085fb-fe92-420f-b1f1-e4e8f8bbe93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = 1 if action == \"버튼0\" else 10 \n",
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58c525-4a88-4c97-abfa-2f1399b42b60",
   "metadata": {},
   "source": [
    "> `reward`라는 용어를 기억할 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0252e8-6094-4dfe-9b38-f753af36adc1",
   "metadata": {},
   "source": [
    "`-` 아무버튼이나 10번정도 눌러보면서 데이터를 쌓아보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "047ed7c3-8764-4ddd-bf0d-d19f819579f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼0 1\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼0 1\n",
      "버튼0 1\n",
      "버튼1 10\n",
      "버튼0 1\n"
     ]
    }
   ],
   "source": [
    "action_space = ['버튼0','버튼1']\n",
    "for _ in range(10):\n",
    "    action = np.random.choice(action_space)\n",
    "    reward = 1 if action == \"버튼0\" else 10\n",
    "    print(action,reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc8896-fdeb-4a85-9909-5e6b75b95939",
   "metadata": {},
   "source": [
    "`-` 깨달았음: `버튼0`을 누르면 1점을 받고, `버튼1`을 누르면 10점을 받는 \"환경(environment)\"이구나? $\\to$ `버튼1`을 누르는 \"동작(=action)\"을 해야하는 상황이구나? \n",
    "\n",
    "- 여기에서 $\\to$의 과정을 체계화 시킨 학문이 강화학습 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6563e7a-ad0a-4275-aa24-72ba7ad0a5ed",
   "metadata": {},
   "source": [
    "> `environment`라는 용어를 기억할 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c411be6-725e-4437-861e-637a74d9d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n",
      "버튼1 10\n"
     ]
    }
   ],
   "source": [
    "action_space = ['버튼0','버튼1']\n",
    "for _ in range(10):\n",
    "    action = '버튼1'\n",
    "    reward = 1 if action == \"버튼0\" else 10\n",
    "    print(action,reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406b3a44-3911-4a18-8a27-9fd8800aef51",
   "metadata": {},
   "source": [
    "- 게임 클리어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561c0fa-0aad-4bdc-860f-5c17b06e2edb",
   "metadata": {},
   "source": [
    "`-` 강화학습: 환경(environment)을 이해 $\\to$ 에이전트(agent)가 행동(action)을 결정 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da34f3b1-63bf-4657-837f-f92cf699fa49",
   "metadata": {
    "tags": []
   },
   "source": [
    "> `agent`라는 용어를 기억할 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9999280-9abd-40b1-b126-742d425a35cb",
   "metadata": {},
   "source": [
    "***위의 과정이 잘 되었다는 의미로 사용하는 문장들*** \n",
    "\n",
    "- 강화학습이 성공적으로 잘 되었다. \n",
    "- 에이전트가 환경의 과제를 완료했다. \n",
    "- 에이전트가 환경에서 성공적으로 학습했다. \n",
    "- 에이전트가 올바른 행동을 학습했다. \n",
    "- 게임 클리어 (비공식) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cbb658-4e99-467b-82b3-9268a3b3b5e6",
   "metadata": {},
   "source": [
    "`-` 게임이 클리어 되었다는 것을 의미하는 지표를 정하고 싶다. \n",
    "\n",
    "- 첫 생각: `버튼1`을 누르는 순간 게임클리어로 보면 되지 않나?\n",
    "- 두번째 생각: 아니지? 우연히 누를수도 있잖아?\n",
    "- 게임클리어조건: (1) 20번은 그냥 진행 (2) 최근 20번의 보상의 평균이 9.5점 이상이면 게임이 클리어 되었다고 생각하자.^[`버튼1`을 눌러야 하는건 맞지만 몇번의 실수는 눈감아 주자는 의미]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548906ca-0d36-42cc-919c-c4d63e1d1daa",
   "metadata": {},
   "source": [
    "`-` 원시코드1: 환경을 이해하지 못한 에이전트 -- 게임을 클리어할 수 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4b8d98a-4bfa-492f-b91d-8c90776bd395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시도:1\t행동:0\t보상:1\t최근20번보상평균:1.0000\t\n",
      "시도:2\t행동:0\t보상:1\t최근20번보상평균:1.0000\t\n",
      "시도:3\t행동:0\t보상:1\t최근20번보상평균:1.0000\t\n",
      "시도:4\t행동:1\t보상:10\t최근20번보상평균:3.2500\t\n",
      "시도:5\t행동:0\t보상:1\t최근20번보상평균:2.8000\t\n",
      "시도:6\t행동:1\t보상:10\t최근20번보상평균:4.0000\t\n",
      "시도:7\t행동:0\t보상:1\t최근20번보상평균:3.5714\t\n",
      "시도:8\t행동:1\t보상:10\t최근20번보상평균:4.3750\t\n",
      "시도:9\t행동:0\t보상:1\t최근20번보상평균:4.0000\t\n",
      "시도:10\t행동:0\t보상:1\t최근20번보상평균:3.7000\t\n",
      "시도:11\t행동:1\t보상:10\t최근20번보상평균:4.2727\t\n",
      "시도:12\t행동:0\t보상:1\t최근20번보상평균:4.0000\t\n",
      "시도:13\t행동:1\t보상:10\t최근20번보상평균:4.4615\t\n",
      "시도:14\t행동:1\t보상:10\t최근20번보상평균:4.8571\t\n",
      "시도:15\t행동:1\t보상:10\t최근20번보상평균:5.2000\t\n",
      "시도:16\t행동:0\t보상:1\t최근20번보상평균:4.9375\t\n",
      "시도:17\t행동:0\t보상:1\t최근20번보상평균:4.7059\t\n",
      "시도:18\t행동:0\t보상:1\t최근20번보상평균:4.5000\t\n",
      "시도:19\t행동:1\t보상:10\t최근20번보상평균:4.7895\t\n",
      "시도:20\t행동:0\t보상:1\t최근20번보상평균:4.6000\t\n",
      "--\n",
      "시도:21\t행동:1\t보상:10\t최근20번보상평균:5.0500\t\n",
      "시도:22\t행동:0\t보상:1\t최근20번보상평균:5.0500\t\n",
      "시도:23\t행동:1\t보상:10\t최근20번보상평균:5.5000\t\n",
      "시도:24\t행동:1\t보상:10\t최근20번보상평균:5.5000\t\n",
      "시도:25\t행동:0\t보상:1\t최근20번보상평균:5.5000\t\n",
      "시도:26\t행동:0\t보상:1\t최근20번보상평균:5.0500\t\n",
      "시도:27\t행동:1\t보상:10\t최근20번보상평균:5.5000\t\n",
      "시도:28\t행동:1\t보상:10\t최근20번보상평균:5.5000\t\n",
      "시도:29\t행동:1\t보상:10\t최근20번보상평균:5.9500\t\n",
      "시도:30\t행동:1\t보상:10\t최근20번보상평균:6.4000\t\n",
      "시도:31\t행동:1\t보상:10\t최근20번보상평균:6.4000\t\n",
      "시도:32\t행동:1\t보상:10\t최근20번보상평균:6.8500\t\n",
      "시도:33\t행동:1\t보상:10\t최근20번보상평균:6.8500\t\n",
      "시도:34\t행동:1\t보상:10\t최근20번보상평균:6.8500\t\n",
      "시도:35\t행동:0\t보상:1\t최근20번보상평균:6.4000\t\n",
      "시도:36\t행동:0\t보상:1\t최근20번보상평균:6.4000\t\n",
      "시도:37\t행동:1\t보상:10\t최근20번보상평균:6.8500\t\n",
      "시도:38\t행동:1\t보상:10\t최근20번보상평균:7.3000\t\n",
      "시도:39\t행동:1\t보상:10\t최근20번보상평균:7.3000\t\n",
      "시도:40\t행동:1\t보상:10\t최근20번보상평균:7.7500\t\n",
      "시도:41\t행동:0\t보상:1\t최근20번보상평균:7.3000\t\n",
      "시도:42\t행동:1\t보상:10\t최근20번보상평균:7.7500\t\n",
      "시도:43\t행동:0\t보상:1\t최근20번보상평균:7.3000\t\n",
      "시도:44\t행동:0\t보상:1\t최근20번보상평균:6.8500\t\n",
      "시도:45\t행동:0\t보상:1\t최근20번보상평균:6.8500\t\n",
      "시도:46\t행동:1\t보상:10\t최근20번보상평균:7.3000\t\n",
      "시도:47\t행동:1\t보상:10\t최근20번보상평균:7.3000\t\n",
      "시도:48\t행동:0\t보상:1\t최근20번보상평균:6.8500\t\n",
      "시도:49\t행동:0\t보상:1\t최근20번보상평균:6.4000\t\n",
      "시도:50\t행동:1\t보상:10\t최근20번보상평균:6.4000\t\n"
     ]
    }
   ],
   "source": [
    "action_space = [0,1]\n",
    "actions = []\n",
    "rewards = []\n",
    "for t in range(1,51):\n",
    "    action = np.random.choice(action_space)\n",
    "    reward = 1 if action == 0 else 10\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    #--#\n",
    "    print(\n",
    "        f\"시도:{t}\\t\"\n",
    "        f\"행동:{action}\\t\"\n",
    "        f\"보상:{reward}\\t\"\n",
    "        f\"최근20번보상평균:{np.mean(rewards[-20:]):.4f}\\t\"\n",
    "    )\n",
    "    if t<20:\n",
    "        pass \n",
    "    elif t==20:\n",
    "        print(\"--\")\n",
    "    else: \n",
    "        if np.mean(rewards[-20:]) > 9.5:\n",
    "            print(\"Game Clear\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107ee6c-0ce6-4d29-a85f-05e8bb474028",
   "metadata": {},
   "source": [
    "`-` 원시코드2: 환경을 깨달은 에이전트 -- 게임클리어 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19ce8841-5909-49d3-a43d-d361a2950d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시도:1\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:2\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:3\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:4\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:5\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:6\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:7\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:8\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:9\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:10\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:11\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:12\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:13\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:14\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:15\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:16\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:17\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:18\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:19\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:20\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "--\n",
      "시도:21\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "Game Clear\n"
     ]
    }
   ],
   "source": [
    "action_space = [0,1]\n",
    "actions = []\n",
    "rewards = []\n",
    "for t in range(1,51):\n",
    "    action = 1\n",
    "    reward = 1 if action == 0 else 10\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    #--#\n",
    "    print(\n",
    "        f\"시도:{t}\\t\"\n",
    "        f\"행동:{action}\\t\"\n",
    "        f\"보상:{reward}\\t\"\n",
    "        f\"최근20번보상평균:{np.mean(rewards[-20:]):.4f}\\t\"\n",
    "    )\n",
    "    if t<20:\n",
    "        pass \n",
    "    elif t==20:\n",
    "        print(\"--\")\n",
    "    else: \n",
    "        if np.mean(rewards[-20:]) > 9.5:\n",
    "            print(\"Game Clear\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ca9e16-b1f0-4f68-9733-0bcf3dcb9529",
   "metadata": {},
   "source": [
    "## B. 수정1: `Env` 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274bc6f-9f40-40ca-b00d-f2dc0231bbc4",
   "metadata": {},
   "source": [
    "`-` `Bandit` 클래스 선언 + `.step()` 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fbd588a-45c0-4842-aebe-8aec6b601f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    def step(self,agent_action):\n",
    "        reward = 1 if agent_action == 0 else 10\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5defbf3-27d4-4dc2-a007-f6f0a934e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시도:1\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:2\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:3\t행동:0\t보상:1\t최근20번보상평균:7.0000\t\n",
      "시도:4\t행동:0\t보상:1\t최근20번보상평균:5.5000\t\n",
      "시도:5\t행동:0\t보상:1\t최근20번보상평균:4.6000\t\n",
      "시도:6\t행동:1\t보상:10\t최근20번보상평균:5.5000\t\n",
      "시도:7\t행동:0\t보상:1\t최근20번보상평균:4.8571\t\n",
      "시도:8\t행동:1\t보상:10\t최근20번보상평균:5.5000\t\n",
      "시도:9\t행동:1\t보상:10\t최근20번보상평균:6.0000\t\n",
      "시도:10\t행동:1\t보상:10\t최근20번보상평균:6.4000\t\n",
      "시도:11\t행동:1\t보상:10\t최근20번보상평균:6.7273\t\n",
      "시도:12\t행동:0\t보상:1\t최근20번보상평균:6.2500\t\n",
      "시도:13\t행동:1\t보상:10\t최근20번보상평균:6.5385\t\n",
      "시도:14\t행동:1\t보상:10\t최근20번보상평균:6.7857\t\n",
      "시도:15\t행동:1\t보상:10\t최근20번보상평균:7.0000\t\n",
      "시도:16\t행동:1\t보상:10\t최근20번보상평균:7.1875\t\n",
      "시도:17\t행동:0\t보상:1\t최근20번보상평균:6.8235\t\n",
      "시도:18\t행동:1\t보상:10\t최근20번보상평균:7.0000\t\n",
      "시도:19\t행동:0\t보상:1\t최근20번보상평균:6.6842\t\n",
      "시도:20\t행동:1\t보상:10\t최근20번보상평균:6.8500\t\n",
      "--\n",
      "시도:21\t행동:0\t보상:1\t최근20번보상평균:6.4000\t\n",
      "시도:22\t행동:0\t보상:1\t최근20번보상평균:5.9500\t\n",
      "시도:23\t행동:1\t보상:10\t최근20번보상평균:6.4000\t\n",
      "시도:24\t행동:0\t보상:1\t최근20번보상평균:6.4000\t\n",
      "시도:25\t행동:0\t보상:1\t최근20번보상평균:6.4000\t\n",
      "시도:26\t행동:1\t보상:10\t최근20번보상평균:6.4000\t\n",
      "시도:27\t행동:0\t보상:1\t최근20번보상평균:6.4000\t\n",
      "시도:28\t행동:1\t보상:10\t최근20번보상평균:6.4000\t\n",
      "시도:29\t행동:1\t보상:10\t최근20번보상평균:6.4000\t\n",
      "시도:30\t행동:0\t보상:1\t최근20번보상평균:5.9500\t\n",
      "시도:31\t행동:1\t보상:10\t최근20번보상평균:5.9500\t\n",
      "시도:32\t행동:0\t보상:1\t최근20번보상평균:5.9500\t\n",
      "시도:33\t행동:0\t보상:1\t최근20번보상평균:5.5000\t\n",
      "시도:34\t행동:0\t보상:1\t최근20번보상평균:5.0500\t\n",
      "시도:35\t행동:0\t보상:1\t최근20번보상평균:4.6000\t\n",
      "시도:36\t행동:0\t보상:1\t최근20번보상평균:4.1500\t\n",
      "시도:37\t행동:1\t보상:10\t최근20번보상평균:4.6000\t\n",
      "시도:38\t행동:1\t보상:10\t최근20번보상평균:4.6000\t\n",
      "시도:39\t행동:1\t보상:10\t최근20번보상평균:5.0500\t\n",
      "시도:40\t행동:0\t보상:1\t최근20번보상평균:4.6000\t\n",
      "시도:41\t행동:1\t보상:10\t최근20번보상평균:5.0500\t\n",
      "시도:42\t행동:0\t보상:1\t최근20번보상평균:5.0500\t\n",
      "시도:43\t행동:1\t보상:10\t최근20번보상평균:5.0500\t\n",
      "시도:44\t행동:0\t보상:1\t최근20번보상평균:5.0500\t\n",
      "시도:45\t행동:0\t보상:1\t최근20번보상평균:5.0500\t\n",
      "시도:46\t행동:0\t보상:1\t최근20번보상평균:4.6000\t\n",
      "시도:47\t행동:1\t보상:10\t최근20번보상평균:5.0500\t\n",
      "시도:48\t행동:1\t보상:10\t최근20번보상평균:5.0500\t\n",
      "시도:49\t행동:0\t보상:1\t최근20번보상평균:4.6000\t\n",
      "시도:50\t행동:0\t보상:1\t최근20번보상평균:4.6000\t\n"
     ]
    }
   ],
   "source": [
    "env = Bandit()\n",
    "action_space = [0,1]\n",
    "actions = []\n",
    "rewards = []\n",
    "for t in range(1,51):\n",
    "    action = np.random.choice(action_space)\n",
    "    reward = env.step(action)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    #--#\n",
    "    print(\n",
    "        f\"시도:{t}\\t\"\n",
    "        f\"행동:{action}\\t\"\n",
    "        f\"보상:{reward}\\t\"\n",
    "        f\"최근20번보상평균:{np.mean(rewards[-20:]):.4f}\\t\"\n",
    "    )\n",
    "    if t<20:\n",
    "        pass \n",
    "    elif t==20:\n",
    "        print(\"--\")\n",
    "    else: \n",
    "        if np.mean(rewards[-20:]) > 9.5:\n",
    "            print(\"Game Clear\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790d28a-967e-4f2b-91a6-ddce493881ad",
   "metadata": {},
   "source": [
    "## C. 수정2: `Agent` 구현 (인간지능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7347d6-13e5-486a-8728-dc25cb4ab679",
   "metadata": {},
   "source": [
    "`-` Agent 클래스 설계\n",
    "\n",
    "- 액션을 하고, 본인의 행동과 환경에서 받은 reward를 기억 \n",
    "- `.act()`함수와 `.save_experience()`함수 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07b20e58-cf65-4dd3-8967-42f723f4d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.action_space = [0,1]\n",
    "        self.action = None \n",
    "        self.reward = None\n",
    "        self.actions = []\n",
    "        self.rewards = [] \n",
    "    def act(self):\n",
    "        prob = [0.5, 0.5]\n",
    "        self.action = 1 #np.random.choice(self.action_space,p=prob)\n",
    "    def save_experience(self):\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddaa733-d14d-47e1-b924-92b581f5b172",
   "metadata": {},
   "source": [
    "--- 대충 아래와 같은 느낌으로 코드가 돌아가요 ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88ee49-ff3d-4ba9-b7d4-4894ecfc824b",
   "metadata": {},
   "source": [
    "**시점0**: init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0381129-4060-4926-86e1-9241dc950f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "env = Bandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9228a4ef-6158-4339-9474-da5d804dc87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, [], [])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action, agent.reward, agent.actions, agent.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22571a7-1d3f-40d2-93c0-7e48d7022a51",
   "metadata": {},
   "source": [
    "**시점1**: agent 가 acition을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "220c17f1-0fb2-445b-8c63-5c2ac5b98697",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2aa0b490-0154-4ad4-94dd-43a378726659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, None, [], [])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action, agent.reward, agent.actions, agent.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9dec6-d176-430c-ac54-8491634e1469",
   "metadata": {},
   "source": [
    "**시점2**: env가 agent에게 보상을 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8257dd32-6a78-4086-8785-33c00dd1fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reward = env.step(agent.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "174baf76-fad9-40b7-a7bb-177420c151cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, [], [])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action, agent.reward, agent.actions, agent.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282a3e7-503c-48c8-9b82-4a2cd2474d80",
   "metadata": {},
   "source": [
    "**시점3**: 경험을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "24ad0a83-2544-4263-9004-ade828924610",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_experience()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cdc18916-42ed-45bf-986e-3f0e53251037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, [1], [10])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.action, agent.reward, agent.actions, agent.rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5159a-0a2f-4e58-97dc-fe2f62804c14",
   "metadata": {},
   "source": [
    "-- 전체코드 -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69629e5d-9f16-4a12-b5b4-c7b2d1de51d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시도:1\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:2\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:3\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:4\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:5\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:6\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:7\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:8\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:9\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:10\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:11\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:12\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:13\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:14\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:15\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:16\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:17\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:18\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:19\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:20\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "--\n",
      "시도:21\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "Game Clear\n"
     ]
    }
   ],
   "source": [
    "env = Bandit()\n",
    "agent = Agent()\n",
    "for t in range(1,51):\n",
    "    agent.act()\n",
    "    agent.reward = env.step(agent.action)\n",
    "    agent.save_experience()\n",
    "    #--#\n",
    "    print(\n",
    "        f\"시도:{t}\\t\"\n",
    "        f\"행동:{agent.action}\\t\"\n",
    "        f\"보상:{agent.reward}\\t\"\n",
    "        f\"최근20번보상평균:{np.mean(agent.rewards[-20:]):.4f}\\t\"\n",
    "    )\n",
    "    if t<20:\n",
    "        pass \n",
    "    elif t==20:\n",
    "        print(\"--\")\n",
    "    else: \n",
    "        if np.mean(agent.rewards[-20:]) > 9.5:\n",
    "            print(\"Game Clear\")\n",
    "            break    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28019e8b-76e0-478e-83d6-7d770b3dec4a",
   "metadata": {},
   "source": [
    "## D. 수정3: `Agent` 구현 (인공지능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a2af9-429f-4f41-b7b7-8b16428203f4",
   "metadata": {},
   "source": [
    "`-` 지금까지 풀이의 한계\n",
    "\n",
    "- 사실 강화학습은 \"환경을 이해 $\\to$ 행동을 결정\" 의 과정에서 \"$\\to$\"의 과정을 수식화 한 것이다.\n",
    "- 그런데 지금까지 했던 코드는 환경(environment)를 이해하는 순간 에이전트(agent)가 최적의 행동(action)^[`버튼1`을 누른다]을 **\"직관적으로\"** 결정하였으므로 기계가 스스로 학습을 했다고 볼 수 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb0516-4495-4e18-b7f9-135919a3f653",
   "metadata": {},
   "source": [
    "`-` 에이전트가 데이터를 보고 스스로 학습할 수 있도록 설계 -- 부제: `agent.learn()`을 설계하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc5e69-16ed-4740-addf-ee17d278deba",
   "metadata": {},
   "source": [
    "1. 데이터를 모아서 `q_table` 를 만든다. `q_table`은 아래와 같은 내용을 포함한다. \n",
    "\n",
    "|행동|보상(추정값)|\n",
    "|:--:|:--:|\n",
    "|버튼0 ($=a_0$)|1 ($=q_0$)|\n",
    "|버튼1 ($=a_1$)|10 ($=q_1$)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97eef17-ff8e-4c5f-94d0-4d7bcfdc6668",
   "metadata": {},
   "source": [
    "2. `q_table`을 바탕으로 적절한 정책(=`policy`)을 설정한다. \n",
    "\n",
    "- 이 예제에서는 버튼0과 버튼1을 각각 $\\big(\\frac{q_0}{q_0+q_1},\\frac{q_1}{q_0+q_1}\\big)$ 의 확률로 선택하는 \"정책\"을 이용하면 충분할 듯 \n",
    "- 처음부터 이러한 확률로 선택하기보다 처음 20번정도는 데이터를 쌓기 위해서 행동을 랜덤하게 선택하고, 그 이후에 위에서 제시한 확률값으로 행동을 선택하는게 합리적인듯. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936fadb6-cab1-479f-89a3-9c647f4019a8",
   "metadata": {},
   "source": [
    "> 여기에서 `q_table`, `policy`라는 용어를 기억하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a4618-fc6f-42e8-95fc-48f9d9d24591",
   "metadata": {},
   "source": [
    "`-` `q_table`을 계산하는 코드 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cccaa9c0-d474-4f7e-9574-d8965ac2bfc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.actions = [0, 1, 1,  0, 1,   0, 0] \n",
    "agent.rewards = [1, 9, 10, 1, 9.5, 1, 1.2] \n",
    "actions = np.array(agent.actions)\n",
    "rewards = np.array(agent.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3a0cd0d3-3ee2-433c-ab85-ecf86aabdeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "q0,q1 = rewards[actions==0].mean(), rewards[actions==1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "61b936fc-fe8f-4ca2-a690-7c8fc0025230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.05, 9.5 ])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.array([q0,q1])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b0dc5fd7-5aa8-4590-8ef4-dcecab25f8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09952607, 0.90047393])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table/ q_table.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "49632925-f681-4b05-9adb-87a5c6d09286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = q_table/ q_table.sum()\n",
    "agent.action = np.random.choice(agent.action_space,p = prob )\n",
    "agent.action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3a96b-e7a2-43d9-bd7e-23c5055f51d7",
   "metadata": {},
   "source": [
    "`-` 최종코드정리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "03f53425-b31a-4f36-a540-566852a9b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.action_space = [0,1]\n",
    "        self.action = None \n",
    "        self.reward = None\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.q_table = np.array([0.001,0.001])\n",
    "        self.n_experience = 0 \n",
    "    def act(self):\n",
    "        if self.n_experience <= 20:\n",
    "            self.action = np.random.choice(self.action_space)\n",
    "        else: \n",
    "            prob = self.q_table/ self.q_table.sum()            \n",
    "            self.action = np.random.choice(self.action_space,p = prob )\n",
    "    def save_experience(self):\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.n_experience = self.n_experience + 1 \n",
    "    def learn(self):\n",
    "        if self.n_experience < 20:\n",
    "            pass \n",
    "        else: \n",
    "            actions = np.array(self.actions)\n",
    "            rewards = np.array(self.rewards)      \n",
    "            q0,q1 = rewards[actions==0].mean(), rewards[actions==1].mean()\n",
    "            self.q_table = np.array([q0,q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e337fb3b-c6ac-470f-baa6-b871a8672b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시도:1\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:2\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:3\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:4\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:5\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:6\t행동:1\t보상:10\t최근20번보상평균:10.0000\t\n",
      "시도:7\t행동:0\t보상:1\t최근20번보상평균:8.7143\t\n",
      "시도:8\t행동:1\t보상:10\t최근20번보상평균:8.8750\t\n",
      "시도:9\t행동:1\t보상:10\t최근20번보상평균:9.0000\t\n",
      "시도:10\t행동:1\t보상:10\t최근20번보상평균:9.1000\t\n",
      "시도:11\t행동:1\t보상:10\t최근20번보상평균:9.1818\t\n",
      "시도:12\t행동:1\t보상:10\t최근20번보상평균:9.2500\t\n",
      "시도:13\t행동:0\t보상:1\t최근20번보상평균:8.6154\t\n",
      "시도:14\t행동:1\t보상:10\t최근20번보상평균:8.7143\t\n",
      "시도:15\t행동:0\t보상:1\t최근20번보상평균:8.2000\t\n",
      "시도:16\t행동:1\t보상:10\t최근20번보상평균:8.3125\t\n",
      "시도:17\t행동:1\t보상:10\t최근20번보상평균:8.4118\t\n",
      "시도:18\t행동:1\t보상:10\t최근20번보상평균:8.5000\t\n",
      "시도:19\t행동:0\t보상:1\t최근20번보상평균:8.1053\t\n",
      "시도:20\t행동:1\t보상:10\t최근20번보상평균:8.2000\t\n",
      "--\n",
      "시도:21\t행동:1\t보상:10\t최근20번보상평균:8.2000\t\n",
      "시도:22\t행동:0\t보상:1\t최근20번보상평균:7.7500\t\n",
      "시도:23\t행동:1\t보상:10\t최근20번보상평균:7.7500\t\n",
      "시도:24\t행동:1\t보상:10\t최근20번보상평균:7.7500\t\n",
      "시도:25\t행동:1\t보상:10\t최근20번보상평균:7.7500\t\n",
      "시도:26\t행동:1\t보상:10\t최근20번보상평균:7.7500\t\n",
      "시도:27\t행동:1\t보상:10\t최근20번보상평균:8.2000\t\n",
      "시도:28\t행동:1\t보상:10\t최근20번보상평균:8.2000\t\n",
      "시도:29\t행동:1\t보상:10\t최근20번보상평균:8.2000\t\n",
      "시도:30\t행동:1\t보상:10\t최근20번보상평균:8.2000\t\n",
      "시도:31\t행동:1\t보상:10\t최근20번보상평균:8.2000\t\n",
      "시도:32\t행동:1\t보상:10\t최근20번보상평균:8.2000\t\n",
      "시도:33\t행동:1\t보상:10\t최근20번보상평균:8.6500\t\n",
      "시도:34\t행동:1\t보상:10\t최근20번보상평균:8.6500\t\n",
      "시도:35\t행동:1\t보상:10\t최근20번보상평균:9.1000\t\n",
      "시도:36\t행동:1\t보상:10\t최근20번보상평균:9.1000\t\n",
      "시도:37\t행동:1\t보상:10\t최근20번보상평균:9.1000\t\n",
      "시도:38\t행동:1\t보상:10\t최근20번보상평균:9.1000\t\n",
      "시도:39\t행동:1\t보상:10\t최근20번보상평균:9.5500\t\n",
      "Game Clear\n"
     ]
    }
   ],
   "source": [
    "env = Bandit()\n",
    "agent = Agent()\n",
    "for t in range(1,51):\n",
    "    # step1: 행동\n",
    "    agent.act()\n",
    "    # step2: 보상\n",
    "    agent.reward = env.step(agent.action)\n",
    "    # step3: 저장 & 학습\n",
    "    agent.save_experience()\n",
    "    agent.learn()    \n",
    "    #--#\n",
    "    print(\n",
    "        f\"시도:{t}\\t\"\n",
    "        f\"행동:{agent.action}\\t\"\n",
    "        f\"보상:{agent.reward}\\t\"\n",
    "        f\"최근20번보상평균:{np.mean(agent.rewards[-20:]):.4f}\\t\"\n",
    "    )\n",
    "    if t<20:\n",
    "        pass \n",
    "    elif t==20:\n",
    "        print(\"--\")\n",
    "    else: \n",
    "        if np.mean(agent.rewards[-20:]) > 9.5:\n",
    "            print(\"Game Clear\")\n",
    "            break    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
